\documentclass{article}
\usepackage{bbold}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\setlength{\columnseprule}{1pt}
\usepackage{cmap}
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[english, russian]{babel}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{mathdots}
\usepackage{xfrac}


\def\columnseprulecolor{\color{black}}

\graphicspath{ {./resources/} }


\usepackage{listings}
\usepackage{xcolor}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{extendedchars=\true}
\lstset{style=mystyle}

\newcommand\0{\mathbb{0}}
\newcommand{\eps}{\varepsilon}
\newcommand\overdot{\overset{\bullet}}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\re}{Re}
\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\Arg}{Arg}
\DeclareMathOperator{\const}{const}
\DeclareMathOperator{\rg}{rg}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\alt}{alt}
\DeclareMathOperator{\Sim}{sim}
\DeclareMathOperator{\inv}{inv}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\argmax}{argmax}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\med}{med}
\newcommand\1{\mathbb{1}}
\newcommand\ul{\underline}
\newcommand{\ppart}[2]{\frac{\partial #1}{\partial #2}}
\renewcommand{\bf}{\textbf}
\renewcommand{\it}{\textit}
\newcommand\vect{\overrightarrow}
\newcommand{\nm}{\operatorname}
\DeclareMathOperator{\df}{d}
\DeclareMathOperator{\tr}{tr}
\newcommand{\bb}{\mathbb}
\newcommand{\lan}{\langle}
\newcommand{\ran}{\rangle}
\newcommand{\an}[2]{\lan #1, #2 \ran}
\newcommand{\fall}{\forall\,}
\newcommand{\ex}{\exists\,}
\newcommand{\lto}{\leftarrow}
\newcommand{\xlto}{\xleftarrow}
\newcommand{\rto}{\rightarrow}
\newcommand{\xrto}{\xrightarrow}
\newcommand{\uto}{\uparrow}
\newcommand{\dto}{\downarrow}
\newcommand{\lrto}{\leftrightarrow}
\newcommand{\llto}{\leftleftarrows}
\newcommand{\rrto}{\rightrightarrows}
\newcommand{\Lto}{\Leftarrow}
\newcommand{\Rto}{\Rightarrow}
\newcommand{\Uto}{\Uparrow}
\newcommand{\Dto}{\Downarrow}
\newcommand{\LRto}{\Leftrightarrow}
\newcommand{\Rset}{\bb{R}}
\newcommand{\Rex}{\overline{\bb{R}}}
\newcommand{\Cset}{\bb{C}}
\newcommand{\Nset}{\bb{N}}
\newcommand{\Qset}{\bb{Q}}
\newcommand{\Zset}{\bb{Z}}
\newcommand{\Bset}{\bb{B}}
\renewcommand{\ker}{\nm{Ker}}
\renewcommand{\span}{\nm{span}}
\newcommand{\Def}{\nm{def}}
\newcommand{\mc}{\mathcal}
\newcommand{\mcA}{\mc{A}}
\newcommand{\mcB}{\mc{B}}
\newcommand{\mcC}{\mc{C}}
\newcommand{\mcD}{\mc{D}}
\newcommand{\mcJ}{\mc{J}}
\newcommand{\mcT}{\mc{T}}
\newcommand{\us}{\underset}
\newcommand{\os}{\overset}
\newcommand{\ol}{\overline}
\newcommand{\ot}{\widetilde}
\newcommand{\vl}{\Biggr|}
\newcommand{\ub}[2]{\underbrace{#2}_{#1}}
\newcommand{\ob}[2]{\overbrace{#2}^{#1}}
\newcommand{\pat}{\partial}

\def\letus{%
    \mathord{\setbox0=\hbox{$\exists$}%
             \hbox{\kern 0.125\wd0%
                   \vbox to \ht0{%
                      \hrule width 0.75\wd0%
                      \vfill%
                      \hrule width 0.75\wd0}%
                   \vrule height \ht0%
                   \kern 0.125\wd0}%
           }%
}
\DeclareMathOperator*\dlim{\underline{lim}}
\DeclareMathOperator*\ulim{\overline{lim}}

\everymath{\displaystyle}

% Grath
\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{decorations.pathmorphing}
\tikzset{snake/.style={decorate, decoration=snake}}
\tikzset{node/.style={circle, draw=black!60, fill=white!5, very thick, minimum size=7mm}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\title{\hugeМатематическая статистика}
\author{Матвеев Сергей M3338}
\date{5 семестр}
\begin{document}

\maketitle

\section{Предмет математической статистики. Задачи, решаемые методами математической статистики.}
Пусть у нас есть генеральная совокупность, но мы хотим ее как-то изучать, тогда мы можем взять ее часть - выборку.\\
Мы хотим по выборке сделать содержательные вероятностные выводы о генеральной совокупности.\\
Примеры задач, которые могут быть решены таким способом:\\
\begin{enumerate}
    \item Бросок монеты (оценить вероятность орла, честно|нечестно)
    \item Замеры показателя: какие типичные значения для показателя
    \item Как учатся мальчики и девочки (одинаково или по разному)
    \item Цена на недвижимость, расстояние до метро (оценка зависимости)
\end{enumerate}

\section{Модель простейшей выборки. Эмпирическая функция распределения и её свойства. Способы
визуализации выборки.}
\textbf{Простейшая модель выборки} - $X_1, X_2, \dots, X_n$ - $i.d.d.$, $F$ - функция распределения (теоретическая функция).\\
$X_1, \dots, X_n \sim F$ ($F$ мы не знаем априори)\\
$x_1, \dots, x_n$ - реализация выборки\\
\textbf{Цель}: оценить из реализации $x_1, \dots, x_n$ теор $F$.\\
\textbf{Эмпирическая фукнция распределения}:\\
$\mu_n(t) = \displaystyle\sum_{i = 1}^{n} \mathbb{1}(X_i \leq t)$\\
$F_n(t) = \frac{\mu_n(t)}{n}$ - эмпирическая функция распределения.\\
\newpage
\bf{Свойства}\\
$\mathbb{1}(X_i \leq t) \sim Bern(F(t))$\\
$\mathbb{E}(F_n(t)) = F(t)$ (это называется несмещенность)\\
$\Var(F_n(t)) = \frac{F(t)(1 - F(t))}{n}$\\
ЗБЧ: $F_n(t) \xrightarrow{\text{$P$}} F(t)$ - это называется состоятельность\\
ЦПТ: $\frac{\mu_n(t) - nF(t)}{\sqrt{F(t)(1 - F(t))n}} \xrightarrow{d} U \sim N(0,1) = $\\
$= \sqrt{n}\frac{F_n(t) - F(t)}{\sqrt{F(t)(1 - F(t))}}$ (ассимптотическая нормальность)\\
\textbf{Теорема Гливенко-Кантелли}\\
$\underset{t \in \mathbb{R}}{\sup}|F_n(t) - F(t)| \xrightarrow[n \to \infty]{a.s.} 0$\\
\textbf{Теорема Колмогорова}\\
$D_n = \underset{x}{\sup} |F_n(x) - F(x)| \Rightarrow P(\sqrt{n} D_n \leq t) \xrightarrow[n \to \infty]{} K(t) = \displaystyle\sum_{j = -\infty}^{+\infty} (-1)^je^{-2j^2t^2}$\\
$F \in C(\mathbb{R})$\\
Такая функция называется функцией Колмогорова\\
\textbf{Теорема Смирнова}\\
$X_1, \dots, X_n$, $Y_1, \dots, Y_n$ - независимы\\
Обе распределены по $F \in C(\mathbb{R})$\\
$D_{n,m} = \underset{x}{\sup} |F_n(x) - F_m(x)| \Rightarrow P(\sqrt{\frac{mn}{m + n}} D_{n,m} \leq t) \xrightarrow[n \to \infty, m \to \infty]{} K(t)$\\
Стоит отметить, что обе теоремы имеют смысл при $t \geq 0$\\
Выборку можно визуализировать используя гистограмму или box plot\\
\section{Начальные выборочные моменты и их свойства, в том числе выборочное среднее.}
$\alpha_k = EX_1^k$ - к-ый теоретический момент.\\
$\overline{g(X)} = \frac{1}{n}\displaystyle\sum_{k = 1}^{n}g(X_k)$, $g: \mathbb{R} \to \mathbb{R}$\\
$\widehat{\alpha_k} = \overline{X^k} = \frac{1}{n}\displaystyle\sum_{j = 1}^{n}X_j^k$ - к-ый выборочный момент.\\
$E\widehat{\alpha_k} = \alpha_k$ (несмещенность, мы просто воспользовались линейностью математического ожидания)\\
$\Var\widehat{\alpha_k} = \frac{1}{n}\Var(X_1^k) = \frac{1}{n}(EX_1^{2k} - (EX_1^k)^2)$\\
По ЦПТ получаем:\\
$\sqrt{n}\frac{\widehat{\alpha_k} - \alpha_k}{\sqrt{\alpha_{2k} - \alpha_k^2}} \approx N(0,1)$\\
$\sqrt{n}\frac{\widehat{\alpha_k} - \alpha_k}{\sqrt{\widehat{\alpha_{2k}} - \widehat{\alpha_k}^2}} = \sqrt{n}\frac{\widehat{\alpha_k} - \alpha_k}{\sqrt{\alpha_{2k} - \alpha_k^{2}}} \cdot \sqrt{\frac{\alpha_{2k} - \alpha_k^2}{\widehat{\alpha_{2k}} - \widehat{\alpha_k}}}$ - первый множитель по ЦПТ сходится к $N(0,1)$ по распределнию\\
Давайте посмотрим что будет со вторым множителем. Он будет сходиться к 1 по вероятности.\\
Таким обрбазом:\\
$\sqrt{n}\frac{\widehat{\alpha_k} - \alpha_k}{\sqrt{\alpha_{2k} - \alpha_k^{2}}} \cdot \sqrt{\frac{\alpha_{2k} - \alpha_k^2}{\widehat{\alpha_{2k}} - \widehat{\alpha_k}}} \xrightarrow[]{d} N(0,1)$\\
А почему вторая дробь сходится к единице?\\
$\widehat{\alpha_k} \xrightarrow[]{P} \alpha_k$ (по ЗБЦ, это называется состоятельность)\\
$\widehat{\alpha_{2k}} - \widehat{\alpha_k}^2 \xrightarrow[]{P} \alpha_{2k} - \alpha_k^2$\\
$\overline{X}$ - выборочное среднее.\\
\section{Выборочные центральные моменты и их свойства, в том числе выборочная дисперсия. Дельта-
метод.}
$\beta_k = E(X_1 - EX_1)^k$ - к-ый центральный момент.\\
$\widehat{\beta_k} = \overline{(X - \overline{X})^k} = \frac{1}{n}\displaystyle\sum_{j = 1}^{n} (X_j -\overline{X})^k$ - к-ый выборочный момент.\\
$\widehat{\beta_2} = S_{*}^2$ - выборочная дисперсия.\\
$S_{*}$ - выборочное стандартное отклонение (выборочное среднеквадратичное отклонение).\\
$Note:$ выборочные моменты есть ничто иное как моменты посчитанные относительно эмпирического распределения.\\
$S_{*} = \overline{X^2} - (\overline{X}^2)$\\
$\widehat{\beta_k} = Poly(\widehat{\alpha_k}, \dots, \widehat{\alpha_1})$\\
$\widehat{\alpha_1}, \dots, \widehat{\alpha_k}$ - состоятельные оценки (имеет место сходимость по вероятности)\\
Так как полином это непрерывная функция, то $\widehat{\beta_k} \xrightarrow[]{P} \beta_k$\\
$ES_*^2 = \frac{1}{n}\displaystyle\sum_{i = 1}^n EX_i^2 - E(\overline{X})^2 = \alpha_2 - \alpha_1^2 - \frac{\beta_2}{n} = \beta_2 - \frac{\beta_2}{n} = \frac{n - 1}{n}\beta_2$\\
Т.к. $ES_*^2 \neq \beta_2$ такую дисперсию называют несмещенной\\
$S^2 = \frac{n}{n - 1}S_*^2$ - несмещенная дисперсия
\section{Выборочные квантили, в том числе выборочная медиана. Распределение k-ой порядковой
статистики.}
\textbf{Определение. Вариационный ряд}\\
$X_{(1)} \leq X_{(3)} \leq \dots \leq X_{(n)}$ - вариационный ряд\\
\textbf{Определение. Порядковая статистика}\\
$X_{(k)}$ - к-я порядковая статистика.\\
Квантиль порядка $\alpha$\\
$P(X \geq q_\alpha) \geq 1 - \alpha$\\
$P(X \leq \alpha) \geq \alpha$\\
Это общее определение\\
Если $F$ строго возрастает:\\
$F(q_\alpha) = \alpha \Leftrightarrow q_\alpha = F^{-1}(\alpha)$\\
$F^{-1}(\alpha): \sup\{x : F(x) \leq \alpha\}, \inf\{x : F(x) \geq \alpha\}$\\
\textbf{Определение. Выборочный квантиль порядка $\alpha$}\\
$\alpha \in (0,1)$\\
$\exists 0 \leq k \leq n - 1: \frac{k}{n} \leq \alpha < \frac{k + 1}{n}$\\
$X_{(k + 1)}$ - выборочный квантиль порядка $\alpha$\\
$\alpha = 0$ $\min(X)$ - нулевой квартиль\\
$F^{-1} = \sum\{x \in \mathbb{R} F_n(x) \leq \alpha\}$\\
$\alpha = \frac{1}{4}$ - первый вартиль (нижний квартиль)\\
$\alpha = \frac{1}{2}$ - второй квартиль (выборочная медиана)\\
$\alpha = \frac{3}{4}$ - третий квартиль (верхний квартиль)\\
$\alpha = 1$ - $\max(X)$ (четвертый квартиль)\\
$n = 2m \Rightarrow = med(X) = \frac{X_{(m)} + X_{(m + 1)}}{2}$\\
$n = 2m + 1 \Rightarrow med(X) = X_{(m + 1)}$\\
$IQR = \Delta$ между верхним и нижним квартилем.\\
$P(X_{(k)} \leq t) = P(\mu_n(t) \geq k) = \displaystyle\sum_{j = k}^n C_n^j F^j(t)(1 - F(t))^{n - j}$\\
$B(z,a,b) = \frac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)} \displaystyle\int_0^z t^{a - 1}(1 - t)^{b - 1}dt = B(F(t),k,n - k + 1)$\\
$0 \leq z \leq 1$\\
Пусть $p()$ - теоретическая плотность, то есть $p = F'$\\
$(P(X_{(k)} \leq t))_t' = \frac{\Gamma(n + 1)}{\Gamma(k) \Gamma(n - k + 1)} \cdot F^{k - 1}(t)(1 - F(t))^{n - k} \cdot p(t)$ - плотность к-й порядковой статистики\\
\section{Теоремы об асимптотиках среднего и крайнего членов вариационного ряда (идеи доказа-
тельств).}
Средний член вариационного ряда: $\frac{K(n)}{n} \to const \in (0,1)$\\
Крайний член вариационного ряда:\\
$X_{(r)}$, $r$ - огр.\\
$X_{(n + 1 - s)}$, $s$ - огр.\\
\textbf{Теорема об ассимптотике среднего члена вариационного ряда}\\
$0 < \alpha < 1$ - теоретическая плотность.\\
$q_\alpha$ - теоретический квантиль порядка $\alpha$\\
$p \in C^1($ окр-сть $q_\alpha)$\\
$p(q_\alpha) > 0$\\
Тогда:\\
$\sqrt{n} \cdot f(q_\alpha) \frac{X_{(\lfloor n \alpha \rfloor)} - q_\alpha}{\sqrt{\alpha (1 - \alpha)}} \xrightarrow[]{d} N(0, 1)$\\
\textbf{Идея доказательства}\\
Пусть $\lfloor n \alpha \rfloor = k$\\
Мы умеем писать плотность для $X_{(k)}$\\
Затем у нас идет преобразование:\\
$g(x) = \sqrt{n}p(q_\alpha)\frac{x - q_\alpha}{\sqrt{\alpha(1 - \alpha)}} \leadsto p_{g(X_{(k)})}(t) = p_{X_{(k)}}(g^{-1}(t))|g^{-1}(t)_t'|$ (теорема из прошлого семестра)\\
Там вылезут факториалы, от них мы умеем избавляться по Стирлингу\\
Затем надо будет воспользоваться непрерывной дифференцируемостью:\\
$p \in C^1($ окр-сть $q_\alpha)$\\
$p(q_\alpha) > 0$\\
Тогда в пределе наша новая плотность будет стремиться к плотности нормального стандартного закона.\\
\textbf{Теорема об ассимптотике крайних членов вариационного ряда}\\
$r,s,F,x,p()$ - плотность\\
Тогда:\\
$n F(X_{(r)}) \xrightarrow[]{d} \Gamma(r,1)$\\
$n (1 - F(X_{(n + 1 - s)})) \xrightarrow[]{d} \Gamma(s, 1)$\\
И оба распределения независимы.\\
\textbf{Идея доказательства}\\
У нас есть совместная плотность и какое-то преобразование, тогда мы можем написать плотность после преобразования\\
Затем берем предел и мы получим плотность равная произведению двух этих двух законов.
\section{Постановка задачи точечного оценивания параметров. Свойства оценок.}
$X_1, \dots, X_n \sim F_{\theta} \in \Theta \subset \mathbb{R}^d$\\
$\theta$ - некий фиксированный неизвестный вектор.\\
Наша цель оценить $\theta$ в виде $\widehat{\theta} = \widehat{\theta}(X_1, \dots, X_n)$\\
\textbf{Определение. Состоятельность}\\
$\widehat{\theta}$ - состоятельная оценка $\theta \Leftrightarrow \widehat{\theta} \xrightarrow[]{P} \theta$\\
\textbf{Определение. Несмещенность}\\
$b.as(\widehat{\theta}) \stackrel{def}{=} E\widehat{\theta} - \theta$ - смещение\\
$b.as(\widehat{\theta}) = 0 \Leftrightarrow$ несмещенная\\
\textbf{Определение. Ассимптотическая нормальность}\\
$\sqrt{n}(\widehat{\theta} - \theta) \to N(0, \Sigma_{\theta})$\\
\textbf{Определение. Эффективность (оптимальность)}\\
$\widehat{\theta_1}$ эффективнее $\widehat{\theta_2} \Leftrightarrow MSE\widehat{\theta_1} < MSE\widehat{\theta_2}$\\
$MSE\widehat{\theta} = E\norm{\widehat{\theta} - \theta}^2 = E(\widehat{\theta} - \theta)^T(\widehat{\theta} - \theta)$\\
\textbf{Утверждение}\\
$MSE\widehat{\theta} = tr(\Var\widehat{\theta}) + \norm{b.as \widehat{\theta}}^2$\\
\textbf{Доказательство}\\
$MSE = E(\widehat{\theta} - \theta)^T(\widehat{\theta} - \theta) = E(\widehat{\theta} - E\widehat{\theta} + E\widehat{\theta} - \theta)^T(\widehat{\theta} - E\widehat{\theta} + E\widehat{\theta} - \theta) = E(\widehat{\theta} - E\widehat{\theta})^T(\widehat{\theta} - E\widehat{\theta}) = \displaystyle\sum \Var\widehat{\theta_i} + \norm{b.as \widehat{\theta}}^2$
\begin{enumerate}
    \item Ассимптотическая нормальность $\Rightarrow$ состоятельность\\
    $\widehat{\theta} - \theta = \frac{1}{\sqrt{n}}\sqrt{n}(\widehat{\theta} - \theta) \xrightarrow[]{P} 0$\\
    \item Ассимптотическая нормальность $\Rightarrow$ $b.as \widehat{\theta} \to 0$\\
    Пусть $d = 1$\\
    $P(|\theta - E\widehat{\theta}| > \varepsilon) = P(\frac{\sqrt{n}|\theta - E\widehat{\theta}|}{\sigma} > \frac{\varepsilon \sqrt{n}}{\sigma}) = 1 - P(\dots < \frac{\varepsilon \sqrt{n}}{\sigma}) \approx 1 - (2\Phi(\frac{\varepsilon \sqrt{n}}{\sigma}) - 1) = 2(1 - \Phi(\frac{\varepsilon \sqrt{n}}{\sigma})) \to 0$
    \item Состоятельность $\Rightarrow$ $b.as \widehat{\theta} \to 0$\\
    Следует из усиленного закона больших чисел\\
    $\overline{X} \xrightarrow[]{a.s} \mu \Rightarrow E\overline{X} \to \mu$ (По теореме Лебега о мажорируемой сходимости)
    \item Пусть $d = 1$, $b.as \widehat{\theta} \to 0, \Var\widehat{\theta} \to 0 \Rightarrow \widehat{\theta}$ - сост.
\end{enumerate}
\section{Метод моментов.}
Рассмотрим $g_1, \dots, g_d$\\
$\exists Eg_1(X_1) = m_1(\theta_1, \dots, \theta_d)$\\
$\exists Eg_2(X_2) = m_2(\theta_1, \dots, \theta_d)$\\
$\dots$\\
$\exists Eg_d(X_d) = m_d(\theta_1, \dots, \theta_d)$\\
$\begin{cases}
\overline{g_1(X)} = m_1(\widehat{\theta_1}, \dots, \widehat{\theta_d})\\
\dots\\
\overline{g_d(X)} = m_d(\widehat{\theta_1}, \dots, \widehat{\theta_d})
\end{cases}$\\
Пусть $\exists!$ решение:\\
$\begin{cases}
\widehat{\theta_1} = \alpha_1(\overline{g_1(X)}, \dots, \overline{g_d(X)})\\
\dots\\
\widehat{\theta_d} = \alpha_d(\overline{g_1(X)}, \dots, \overline{g_d(X)})
\end{cases}$\\
Тогда это будет оценка методов моментов.\\
\section{Метод максимального правдоподобия.}
probability must function: $p(x, \theta) = p(x|\theta)$\\
probability identity function: $p(x, \theta) = p(x|\theta)$\\
Будем называть оба случая плотностью.\\
Пусть у нас есть выборка $X_1, \dots, X_n \sim p(x|\theta)$\\
$L(x|\theta) = \displaystyle\prod p(x_i|\theta)$ - функция правдоподобия\\
$\theta^* = \underset{\widehat{\theta}}{argmax}(L(x, \theta))$ - оценка максимума правдоподобия\\
$\theta \in \Theta$ - открыто\\
$\theta_1 \neq \theta_2 \Rightarrow L(x,\theta_1) \neq L(x, \theta_2)$\\
\textbf{Доказательство}
\begin{enumerate}
    \item Посмотреть и подумать
    \item Рассмотреть $\ln{L(x|\theta)}$; $\frac{\partial \ln{L(x, \theta)}}{\partial \theta}$
    \item $\frac{\partial \ln{L(x, \theta)}}{\partial \theta} = 0$
    \item Проверить достаточные условия максимума
\end{enumerate}\
\section{Оценки максимального правдоподобия для нормальной и полиномиальной моделей.}
1) $N(\theta_1, \theta_2)$\\
$L(x, \theta) = \displaystyle\prod_{i = 1}^n \frac{1}{\sqrt{2\pi \theta_2}} \exp{(-\frac{(x_i - \theta_1)^2}{2\theta_2}})$\\
$\ln{L(x,\theta)} = \displaystyle\sum_{i = 1}^n [-\frac{1}{2}\ln{2\pi} - \frac{1}{2}\theta_2 - \frac{(x_i - \theta_1)^2}{2\theta_2}]$\\
$\frac{\partial\ln{L(x, \theta_1)}}{\theta_1} = \displaystyle\sum_{i = 1}^n\frac{2(x_i - \theta_1)}{2\theta_2} = \displaystyle\sum_{i = 1}^n\frac{x_i - \theta_1}{\theta_2}$\\
$\frac{\partial\ln{L(x, \theta)}}{\partial \theta_2} = \displaystyle\sum_{i = 1}^n[-\frac{1}{2\theta_2} + \frac{(x_i - \theta_1)^2}{2\theta_2^2}]$\\
$\displaystyle\sum_{i = 1}^n \frac{(x_i - \widehat{\theta_1})}{\widehat{\theta_2}} = 0 \Rightarrow \widehat{\theta_1} = \overline{X}$\\
$\displaystyle\sum_{i = 1}^n [-\frac{1}{2\widehat{\theta_2}} + \frac{(x_i - \widehat{\theta_1})^2}{2\widehat{\theta_2}^2}] = 0 \Rightarrow \widehat{\theta_2} = S_*^2$\\
\\
2) $Poly(1, p): p = (p_1, \dots, p_m)$\\
Рассмотрим частоты:\\
$\nu_1$ - кол-во наблюдений типа 1\\
$\dots$\\
$\nu_m$ - кол-во наблюдений типа $m$\\
Суммируем и смотрим на функцию правдоподобия\\
$L(X, p) = p_1 \dots p_m$\\
$\ln{L(X, p)} = \displaystyle\sum_{j = 1}^{m - 1}\nu_j \ln{p_j} + \nu_m \ln{(1 - p_1 - \dots - p_{m - 1})}$\\
$\frac{\partial \ln{L\dots}}{\partial p_j} = \frac{\nu_j}{p_j} - \frac{\nu_m}{1 - p_1 - \dots - p_{m - 1}} = 0$\\
$\displaystyle\sum$ уравнения: $\nu_j(1 - \widehat{p_1} - \dots - \widehat{p_{m - 1}}) = \widehat{p_j} \cdot \nu_m$\\
$\widehat{p_m}(n - \nu_m) = \nu_m (1 - \widehat{p_m})$\\
$\widehat{p_m}n - \widehat{p_m}\nu_m = \nu_m$\\
$\widehat{p_m} = \frac{\nu_m}{n}$\\
$\widehat{p_j} = \frac{\nu_j \widehat{p_m}}{\nu_m} = \frac{\nu_j}{n}$
\section{Информация Фишера.}
$d = 1: L(X, \theta) = \displaystyle\prod p(X_j, \theta)$\\
$\ln{L(X, \theta)} = \displaystyle\sum \ln{p(x_j, \theta)}$\\
$V(X, \theta) = \frac{\partial \ln{L\dots}}{\partial \theta} = \displaystyle\sum \frac{\partial \ln{p\dots}}{\partial \theta}$ - вклад выборки\\
$\theta \in \Theta$ - открыто\\
$\theta_1 \neq \theta_2 \Rightarrow p(X, \theta_1) \neq p(X, \theta_2)$\\
Регулярность:\\
\begin{enumerate}
    \item $\frac{\partial}{\partial \theta} \displaystyle\int_X T(X) L(X_i, \theta)dX = \displaystyle\int \frac{\partial}{\partial \theta}L(X, \theta) \cdot T(X) dX$\\
    Необходимое условие $\sup p_x$ не зависит от $\theta$\\
    $U[0, \theta]$ $\displaystyle\int_{0}^{\theta} \frac{1}{\theta}dt = 1$\\
    $(\displaystyle\int_0^{\theta} \frac{1}{\theta}dt)_{\theta}' = (\frac{1}{\theta}\displaystyle\int_0^\theta dt)_\theta' = -\frac{1}{\theta^2} \displaystyle\int_0^\theta dt + \frac{1}{\theta} = 0 \neq \displaystyle\int_0^\theta(\frac{1}{\theta})_\theta' dt$
    \item $EV^2(X, \theta) < \infty$
\end{enumerate}
$\displaystyle\int_X L(X, \theta)dX = 1 \xrightarrow[]{\frac{\partial}{\partial \theta}} \displaystyle\int_X \frac{\partial L(.)}{\partial \theta}dX = \displaystyle\int_X \frac{\frac{\partial L(\dots)}{\partial \theta}}{L(\dots)} \cdot L(\dots) dX = \displaystyle\int_X V(X, \theta) L(X, \theta) dX = EV(X, \theta) = 0$\\
$I(\theta) = \Var(V(X_i, \theta)) = E(V^2(X_i, \theta))$ - информация Фишера для всей выборки\\
$V(X, \theta) = \displaystyle\sum_j \frac{\partial \ln{p(x, \theta)}}{\partial \theta} \Rightarrow \Var(V(X, \theta)) = n \cdot \Var \frac{\partial \ln{p(x, \theta)}}{\partial \theta}$\\
$i(\theta)$ - информация Фишера для 1 наблюдения\\
$i(\theta) = E(\frac{\partial \ln{p(x_j, \theta)}}{\partial \theta})^2$\\
$\frac{\partial}{\partial \theta} \displaystyle\int_\mathbb{R} \frac{\partial \ln{p(x, \theta)}}{\partial \theta} \cdot p(x, \theta) dx = \displaystyle\int_\mathbb{R} \frac{\partial^2 \ln{p(x, \theta)}}{\pat^2\theta} dx + \displaystyle\int_\mathbb{R} \frac{\partial \ln{\dots}}{\partial \theta} \frac{\partial p \dots}{\partial \theta}\text{вот тут домножаем и делим на плотность} dx = E \frac{\partial^2 \ln{p(x, \theta)}}{\partial \theta^2} + E(\frac{\partial \ln{p(x, \theta)}}{\partial \theta})^2 = 0$\\
$i(\theta) = E(\frac{\partial \ln{p(x_j, \theta)}}{\partial \theta})^2 = -E\frac{\partial^2 \ln{p(x, \theta)}}{\partial \theta^2}$\\
Произвольное $d$:\\
$i(\theta) = -(E\frac{\partial^2 \ln{p(X, \theta)}}{\partial \theta_i \partial \theta_j})_{1 \leq i, j \leq d}$\\
$I(\theta) = n i(\theta)$\\
\section{Неравенство Рао-Крамера.}
Модель регулярная, d = 1\\
$\tau(\theta)$ - оцениваемая функция\\
$\tau \in C^1$ (как правило $\tau(\theta) = \theta$)\\
$E\widehat{\tau(\theta)} = \theta$\\
Тогда:\\
$\Var \widehat{\tau(\theta)} \geq \frac{[\tau'(\theta)]^2}{n i(\theta)}$\\
$\tau'(\theta) = \displaystyle\int \widehat{\tau(\theta)} \frac{\partial L(X, \theta)}{\partial \theta} dX = \displaystyle\int \widehat{\tau(\theta)}V(X, \theta)L(X, \theta)dX - EV(X, \theta) \cdot E\widehat{\tau(\theta)} = \Cov(V(X, \theta), \widehat{\tau(\theta)})$\\
$\Cov^2(V(X, \theta), \widehat{\tau(\theta)}) \leq \Var(V(X, \theta)) \cdot \Var(\widehat{\tau(\theta)})$\\
\textbf{Многомерный случай}\\
$\tau(\theta): \mathbb{R}^d \to \mathbb{R}$\\
$\tau \in C^1$\\
$E\widehat{\tau{\theta}} = \tau{\theta} \Rightarrow \Var\widehat{\tau(\theta)} \geq \frac{\nabla \tau(\theta)i^{-1}(\theta)\nabla^T\tau(\theta)}{n}$\\
\section{Свойства оценок максимального правдоподобия.}
\bf{Состоятельность}\\
Пусть $\theta_0$ - реальный параметр $\Rightarrow p_{\theta_0}(L(X, \theta_0) > L(X, \theta)) \to 1$\\
$\frac{L(X, \theta)}{L(X, \theta_0)} < 1$\\
$\frac{1}{n}\displaystyle\sum \ln{\frac{p(X_j, \theta)}{p(X_j, \theta)}} < 0$\\
По ЗБЧ $\Rightarrow E_{\theta_0} \ln{\frac{p(x_j, \theta)}{p(x_j, \theta_0)}} \leq E_{\theta_0} [\frac{p(X_j, \theta)}{p(X_j, \theta_0)} - 1] = \displaystyle\int_X p(X, \theta)dX - \displaystyle\int p(X, \theta_0)dX = 0$\\
Давайте введем события:\\
$S_n = \{X : \ln{L(X, \theta_0)} > \ln{L(X, \theta_0 - a)}\} \cap \{X : \ln{L(X, \theta_0)} > \ln{L(X_i \theta_0 + a)}\}$\\
$P_{\theta_0}(S_n) \to 1$\\
$A_n = \{X : |\widehat{\theta} - \theta_0| < a\}$\\
$B_n = \{X : \frac{\partial \ln{L(X, \theta)}}{\partial \theta} |_{\theta = \widehat{\theta}} = 0\}$\\
$S_n \subset A_n B_n \subset A_n \Rightarrow P(A_n) \to 1$\\
\bf{Принцип инвариантности}\\
$\theta \in \Theta \xrto[\varphi]{biection} \gamma \in \Gamma$\\
$\theta = \varphi^{-1}(\theta) \LRto \gamma = \varphi(\theta)$\\
$\underset{\theta}{\sup}L(X, \varphi(\gamma)) = \underset{\gamma}{\sup}L(x, \gamma)$\\
$\gamma* = \varphi(\theta*)$\\
\textbf{Теорема Ассимптотическая нормальность ОМП}\\
Пусть наша модель регулярная, так же пусть:\\
$|\frac{\partial^3 \ln{f(x, \theta)}}{\partial \theta_i \partial \theta_j \partial \theta_k}| \leq M$\\
$\theta_*$ - ОПМ для $\theta$\\
Уравнение $\nabla \ln{L(X, \theta)} = 0$ имеет еддинственное решение.
Тогда:\\
\begin{enumerate}
    \item $\sqrt{n}(\theta_* - \theta) \to N(0, i^{-1}(\theta))$
    \item $\tau(\theta)$ - оцениваемая функция от $\theta$\\
    $\tau \in C^1$\\
    $\sqrt{n}(\tau(\theta_*) - \tau(\theta)) \to N(0, \sigma^2)$\\
    $\sigma^2 = \nabla\tau(\theta)i^{-1}(\theta)\nabla^T\tau(\theta)$
    \item $\sigma^2$ - непрерывная функция от $\theta$ $\Rto \sqrt{\frac{\tau(\theta_*) - \tau(\theta)}{\sigma(\theta_*)}} \to N(0, 1)$
\end{enumerate}
\section{Экспоненциальное семейство распределений.}
Пусть наше распределение относится к экспоненциальному семейству распределений если:\\
$p(x, \theta) = \exp(A(\theta)B(x) + C(\theta) + D(x))$\\
К таким распределениям относятся: $N(), \Gamma(), Pois(), Bin, NB$\\
$\ln{p(x, \theta)} = A(\theta)B(x) + C(\theta) + D(x)$\\
$\frac{\pat \ln{p(x, \theta)}}{\pat \theta} = A'(\theta)B(x) + C'(\theta)$\\
$V(X, \theta) = A'(\theta)\displaystyle\sum B(X_i) + nC'(\theta)$\\
$V(X, \theta) = n(A'(\theta)\overline{B(X)} + C''(\theta))$\\
$\frac{V(X, \theta)}{n} - C'(\theta) = A'(\theta)\overline{B(X)}$\\
$\overline{B(X)} = \frac{V(X, \theta)}{nA'(\theta)} - \frac{C'(\theta)}{A'(\theta)}$\\
$\overline{B(X)}$ - оптимальная оценка для $(-\frac{C'(\theta)}{A'(\theta)})$\\
\section{Байесовские оценки.}
$X_1, \dots, X_n \sim F_\theta$\\
$\theta \sim \pi(\theta)$ - prior\\
$l(\widehat{\theta}, \theta)$ - функция потерь\\
$l(\widehat{\theta}, \theta) = (\widehat{\theta} - \theta)^2$ (default)\\
$R(\widehat{\theta}, \theta) = El(\widehat{\theta}, \theta)$ - риск\\
$r(\widehat{\theta}) = E_{\pi(\theta)}R(\widehat{\theta}, \theta)$ - байесовский риск\\
$\widehat{\theta}_B = \underset{\theta}{argmin}r(\widehat{\theta})$\\
$r(\widehat{\theta}) = El(\widehat{\theta}, \theta)$\\
Давайте вспомним теорему Байеса:\\
$P(A | B) = \frac{P(B | A)P(A)}{P(B)}$\\
$P(\theta | X) = \frac{L(X|\theta)\pi(\theta)}{\displaystyle\int L(X | \theta)\pi(\theta)d\theta}$\\
posterior = likelihoox $\times$ prior\\
$\widehat{\theta}_B = \underset{\widehat{\theta}}{argmin}E[l(\widehat{\theta}) | X]$\\
$r(\theta_*) \leq r(\widehat{\theta})$\\
\section{Минимаксные оценки.}
$m(\widehat{\theta}) = \underset{\theta}{sup} R(\widehat{\theta}, \theta)$\\
$\widehat{\theta_{WC}} = argmin m(\widehat{\theta})$ - минимаксная оценка\\
$r(\widehat{\theta}) \leq m(\widehat{\theta})$\\
\textbf{Утверждение}\\
$\exists \pi(\theta)$ - prior : $R(\widehat{\theta_B}, \theta)$ = const $\Rto \widehat{\theta}_{WC} = \widehat{\theta}_B$\\
\section{Доверительные интервалы. Общая схема построения доверительного интервала. «Универ-
сальный» рецепт.}
\textbf{Определение. Доверительный интервал}\\
$X_1, \dots, X_n \sim F_\theta, \theta \in \Theta \subset \mathbb{R}$\\
$1 - \alpha = \gamma \in (0, 1)$ - уровень доверия\\
default: 0.9, 0.95, 0.99\\
$(T_l(X), T_r(X))$ - доверительный интервал уровня $\gamma = 1 - \alpha$ если $p(\theta \in (T_l(X), T_r(X)) \geq \gamma)$\\
Пусть $T(X, \theta) \sim G$ - не зависит от $\theta$\\
Рассмторим $p(q_1 < T(X, \theta) < q_2) = 1 - \alpha$\\
$q_1 = q_{\frac{\alpha}{2}}$\\
$q_2 = q_{1 - \frac{\alpha}{2}}$\\
Из данной вероятности можно выразить $\theta$\\
\textbf{Универсальный рецепт (нет)}\\
а) $F_\theta(X_k)$\\
$P(F_\theta(X_k) \leq t) = P(X_k \leq F_\theta^{-1}(t)) = F_\theta(F_\theta^{-1}(t)) = t$\\
б) $-\ln{F_\theta(X_k)} \sim Exp(1)$\\
$P(-\ln{U} \leq t) = P(U \geq e^{-t}) = 1 - e^{-t}$\\
в) $-\displaystyle\sum \ln{F_\theta(X_k)} \sim \Gamma(n, 1)$\\
\section{Теорема Фишера и примыкающие к ней леммы. Распределение хи-квадрат.}
\textbf{Лемма о независимости линейной и квадратичной статистик}\\
$X_1, \dots, X_n \sim N(\mu, \sigma^2)$\\
$T = AX, X = (X_1, \dots, X_n)^T A \in M_{m \times n}(\mathbb{R})$\\
$Q = X^T B X, B \in M_n(\mathbb{R}), B = B^T$\\
$AB = 0$\\
Тогда T, Q - независимы\\
\textbf{Доказательство}\\
$\Lambda = U^TBU$\\
$\Lambda = diag(\lambda_1, \dots, \lambda_m, 0, 0)$\\
$\lambda_k$ - собственное число не 0\\
$U = (u_1, \dots, u_n)$ - собственные векторы ортонормированного базиса $\LRto B = U\Lambda U^T = \displaystyle\sum_{j = 1}^m \lambda_j u_j uj^T \Rto Q = \displaystyle\sum_{j = 1}^M \lambda_j (X^TU_j)(U_j^TX) = \displaystyle\sum_j \lambda_j (U_j^T X)^2$\\
$A(\displaystyle\sum_{j = 1}^m \lambda_j u_j u_j^T) = 0$\\
$\displaystyle\sum_{j = 1}^m \lambda_j A U_j u_j^T = 0$\\
Зафиксируем $1 \leq k \leq m$ домножим справа на $u_k$\\
$Au_k = 0 \Rto \forall i A[i, *]u_k = 0$\\
Нам надо доказать, что $\forall i, k A[i, *]X$ и $u_k^TX$ - нез\\
$\Cov(A[i, *]X, u_k^TX) = \Cov(A[0, *]X, X^Tu_k) = A[i, *] \Var X u_k = \sigma^2 A[i, *]u_k = 0$\\
\textbf{Лемма о независимости двух квадратичных статистик}\\
$Q_1 = X^T B_1 X$\\
$Q_2 = X^TB_2X$\\
$B_1 B_2 = B_2 B_1 = 0$\\
Тогда $Q_1, Q_2$ - нез\\
\textbf{Определение $X_n$ - квадратичная}\\
$X_1, \dots, X_n \sim N(0, 1)$\\
$\displaystyle\sum_{k = 1}^{n}X_k^2 \sim \Xi^2(n)$ (распределение)\\
хи-квадрат с n степенями свободы\\
\textbf{Лемма о распределении квадратичной статистики}\\
$X_1, \dots, X_n \sim N(0, 1)$\\
$Q = X^TBX$\\
$B = B^2$\\
Тогда $Q \sim \Xi^2(r), r = rank(B) = tr(B)$\\
$Q = \displaystyle\sum_{k = 1}^n (u_k^T X)^2 \sim \Xi^2(r)$\\
\bf{Доказательство}\\
$u_k^T \sim N(u_k^T EX, u_k^T I_nu_k) = N(0, 1)$\\
$\Cov(u_k^TX, u_j^TX) = 0$\\
$B = U \Lambda U^T$\\
$rank B = rank \Lambda = tr \Lambda$\\
$tr B = tr(U \Lambda U^T)$\\
Заметим что $B_{j, j} = \lambda_j u_j u_j^T = \lambda_j$\\
\textbf{Теорема Фишера}\\
$X_1, \dots, X_n \sim N(\mu, \sigma^2) \Rto$\\
\begin{enumerate}
    \item $\overline{X} \sim N(\mu, \frac{\sigma^2}{n})$
    \item $\frac{n S_*^2}{\sigma^2} = \frac{(n - 1)S^2}{\sigma^2} \sim \Xi^2(n - 1)$
    \item $S^2, \overline{X}$ - нез
    \item $S_*^2, \overline{X}$ - нез
\end{enumerate}
$Y_j = \frac{X_j - \mu}{\sigma}$\\
$\overline{Y} = \frac{1}{\sigma}(\overline{X} - \mu)$\\
$S_*^2(Y) = \frac{1}{n} \displaystyle\sum_{j = 1}^n (Y_j - \overline{Y})^2 = \frac{S_*^2(X)}{\sigma^2}$\\
$\overline{Y} = \frac{\sum Y_j}{n} = (\frac{1}{n}, \dots, \frac{1}{n}) (= b) (Y_1, \dots, Y_n)^T = bY$\\
$nS_*^2(Y) = (Y - BY)^T(Y - BY) = Y^T(I - B)^T(I - B)Y = \sum \Xi^2(tr(I - B))$ - по предыдущей лемме\\
Для того чтобы доказать третье утверждение\\
$b(I - B) = b - b = 0$\\
Тогда мы пользуемся первой леммой\\
Таким образом теорема Фишера доказана.\\
\section{Распределения Фишера, Стьюдента. Построение доверительных интервалов для параметров
нормального закона.}
\textbf{Определение. Распреление Стьюдента}\\
$X_0, X_1, \dots, X_n$ - нез, $N(0, 1)$\\
$\frac{X_0}{\sqrt{\frac{1}{n}\displaystyle\sum_{k = 1}^n X_k^2}} \sim T(n)$\\
n - степени свободы (deg of freedom)\\
Давайте выведем статистику:\\
$\frac{\sqrt{n}\frac{\overline{X} - \mu}{\sigma}}{\sqrt{\frac{1}{n - 1}\frac{n s_*^2}{\sigma^2}}} = \sqrt{n - 1}\frac{\overline{X} - \mu}{S_*} \sim T(n - 1)$\\
$\frac{\sqrt{n}\frac{\overline{X} - \mu}{\sigma}}{\sqrt{\frac{1}{n - 1} \frac{(n - 1)S^2}{\sigma^2}}} = \sqrt{n}\frac{\overline{X} - \mu}{S}$\\
Доверительный интервал:\\
$\overline{X} - \frac{q_{1 - \frac{\alpha}{2}S}}{\sqrt{n}}, \overline{X} + \frac{q_{1 - \frac{\alpha}{2}S}}{\sqrt{n}}$\\
\textbf{Определение. Распределение Фишера}\\
$\Xi_n^2 \sim \Xi^2(n)$\\
$\Xi_m^2 \sim \Xi^2(m)$\\
Они независимы\\
$\frac{\frac{\Xi_n^2(n)}{n}}{\frac{\Xi_m^2(m)}{m}} \sim F(n, m)$\\
Мы доказали теорему Фишера давайте теперь с помощью теоремы мы рассмотрим задачу построения доверительных интервалов нормального закона\\
$P(-q_{1 - \frac{\alpha}{2}} \leq \frac{X - \mu}{\sigma^2} \leq q_{1 - \frac{\alpha}{2}})$ - ну и потом просто выражаем\\
\begin{itemize}
    \item $\sigma^2$ - известно, $\mu = ?$\\
    Рассмотрим два варианта: $\frac{X_1 - \mu}{\sigma} \sim N(0, 1)$\\
    $\sqrt{n}\frac{\overline{X} - \mu}{\sigma} \sim N(0, 1)$\\
    Доверительный интервал уровня $1 - \alpha$ $[\overline{X} - \frac{q_{1 - \frac{\alpha}{2}\sigma}}{\sqrt{n}}, \overline{X} + \frac{q_{1 - \frac{\alpha}{2}\sigma}}{\sqrt{n}}]$
    \item $\mu$ - известно, $\sigma^2 = ?$\\
    $-q \leq \sqrt{n}\frac{(\overline{X} - \mu)}{\sigma} \leq q$\\
    $-q\sigma \leq \sqrt{n}(\overline{X} - \mu) \leq q\sigma$\\
    $\frac{\sqrt{n}(\overline{X} - \mu)}{q} \leq \sigma$\\
    $-\frac{\sqrt{n}(\overline{X} - \mu)}{q} \leq \sigma$
\end{itemize}
Рассмотрим следующую статистику:\\
$\displaystyle\sum \frac{(X_i - \mu)^2}{\sigma^2} \sim \Xi^2(n)$\\
$P(q_{\frac{\alpha}{2}} \leq \displaystyle\sum \frac{(X_i - \mu)^2}{\sigma^2} \leq q_{1 - \frac{\alpha}{2}}) = 1 - \alpha$\\
Доверительный интервал:\\
$\frac{\displaystyle\sum (X_i - \mu)^2}{q_{1 - \frac{\alpha}{2}}} \leq  \sigma^2 \leq \frac{\displaystyle\sum(X_i - \mu)^2}{q_{\frac{\alpha}{2}}}$\\
Давайте теперь рассмотрим задачу построения доверительного интревала $\mu = ?, \sigma^2 = ?$\\
Воспользуемся теоремой Фишера:\\
$\frac{n S_*^2}{\sigma^2} \sim \Xi^2(n - 1)$\\
Доверительный интервал: $\frac{n S_*^2}{q_{1 - \frac{\alpha}{2}}} \leq \sigma^2 \leq \frac{n S_*^2}{q_{\frac{\alpha}{2}}}$\\
\section{Асимптотические доверительные интервалы. Доверительный интервал для математического
ожидания, дисперсии, медианы.}
Раньше мы говорили $P(\theta \in (l_n, r_n)) \geq 1 - \alpha$\\
Таперь же мы будем говорить $\lim_{n \to \infty} P(\theta \in (l_n, r_n)) \geq 1 - \alpha$\\
$T(X, \theta) \xrto[]{d} G$ не зависит от $\theta$\\
ЦПТ и ее следствия\\
$\sqrt{n}\frac{\overline{X} - \mu}{\sigma} \to N(0, 1)$\\
$\sqrt{n}\frac{\overline{X} - \mu}{S} \to N(0, 1)$\\
Доверительный интервал: $\overline{X} \pm \frac{q_{1 - \frac{\alpha}{2}}S}{\sqrt{n}}$\\
$\sqrt{n}\frac{S_*^2 - \sigma^2}{\sqrt{\widehat{\beta_4} - S_*^4}} \to N(0, 1)$\\
Доверительный интервал: $S_*^2 \pm \frac{q_{1 - \frac{\alpha}{2}}\sqrt{\widehat{\beta_4} - S_*^4}}{\sqrt{n}}$\\
Теорема об ассимтотике среднего члена вариационного ряда\\
$\sqrt{n}\frac{X_{(\lfloor np \rfloor)} - q_p}{\sqrt{p(1 - p)}} f(q_p) \to N(0, 1)$\\
Доверительный интервал для медианы: $p = \frac{1}{2}$\\
$\sqrt{n}f(q_p)\frac{X_{(\lfloor \frac{n}{2} \rfloor)} - q_p}{\frac{1}{2}}$ (зачастую f это константа)\\
Доверительный интервал: $X_{(\lfloor \frac{n}{2}) \rfloor} \pm \frac{q_{1 - \frac{\alpha}{2}}}{\sqrt{n} \cdot const}$\\
\section{Постановка задачи проверки статистических гипотез: выбор нулевой и альтернативной гипотез, общий принцип работы статистического теста, p-value, ошибки I и II рода (false positive and false negative).}
Нам надо будет выделить основное предположение (по умолчанию) и альтернативное предположение (наше подозрение или то, что мы хотим доказать)\\
Давайте рассуждать:\\
рациональное, с точки зрения инопланетянина (не опираться на жизненный опыт)\\
$X_1, \dots, X_n$ - выборка в широком смысле.\\
$(X_1, \dots, X_n) \sim F$\\
$H_0$ - нулевая гипотеза.\\
$H_1$ - альтернатива.\\
Так же пусть нам дали уровень значимости\\
$\alpha \in (0, 1)$ (по умолчанию 0.1, 0.05, 0.01, 0.001)\\
Статистический тест (критерий)\\
$\delta(X, \alpha, H_0, H_1) = \begin{cases}
    accept H_0\\
    reject H_0 (w.\text{ } respect\text{ to } H_1)
\end{cases}$
То есть в первом случае данные не противоречат $H_0$, а втором противоречат.\\
Но это не значит, что мы доказали утверждение.\\
Пусть у нас есть функция $T(X)$ - статистика критерия\\
$T(X)$ либо в точности, либо в пределе стремится к $G$ при условии $H_0 (\sim \text{ or } \to)$\\
$P(T(X) \in T_0(\alpha) | H_0) = 1 - \alpha$\\
$\text{if } T(x) \in T_1(\alpha): \text{ reject } H_0$\\
$\text{else: accept} H_0$\\
$T_0(\alpha)$ - область принятия\\
$T_1(\alpha)$ - область опровержения\\
\begin{enumerate}
    \item left: $T_0(\alpha) = [q_\alpha, +\infty)$
    \item right: $T_0(\alpha) = (-\infty, q_\alpha]$
    \item two: $T_0(\alpha) = [q_{\frac{\alpha}{2}}, q_{1 - \frac{\alpha}{2}}], T_1(\alpha) = \overline{T_0(\alpha)}$
\end{enumerate}
$p_l = P(U \leq T(x) | H_0)$\\
$p_r = P(U > T(x) | H_0)$\\
$p = 2min(p_l, p_r)$\\
$\text{if p < } \alpha: \text{ reject }H_0$\\
$\text{else: accept }H_1$\\
Если мы опровергли нулевую гипотезу, но она верна, то это ошибка первого рода (false positive), ее максимальная вероятность это в точности $\alpha$\\
Если мы не опровергли нулевую гипотезу, но она была не верна, то это ошибка второго рода (false negative), ее вероятность это $\beta$\\
$\beta = P(T(X) \in T_0(\alpha) | H_1)$\\
\section{Статистические тесты, основанные на доверительных интервалах (z-тест для одной/двух
выборок, t-тест для одной/двух выборок, F-тест).}
Когда мы строили доверительные интервалы то мы зажимали статистику между квантилями. Это похоже на двухсторонний тест.\\
$X_1, \dots, X_n \sim F_{\theta}$\\
$T(X, \theta) \to U \sim G$\\
$P(q_{\frac{\alpha}{2}} \leq T(X, \theta) \leq q_{1 - \frac{\alpha}{2}}) = 1 - \alpha$\\
$H_0 : \theta = \theta_0$\\
$P(T(X, \theta_0) \in T_0(\alpha) | \theta = \theta_0) = 1 - \alpha$\\
$H_1 = \theta \neq \theta_0, \theta > \theta_0, \theta < \theta_0$\\
Пример:\\
1) $X_1, \dots, X_n \sim F, \mu = EX_1, \exists \Var X$\\
$H_0: \mu = \mu_0$\\
$T(X) = \sqrt{n} \frac{\overline{X} - \mu_0}{S} \to N(0, 1)$ если $\mu = \mu_0$\\
При $H_1: \mu \neq \mu_0$ у нас двухсторонняя критическая область\\
При $H_1: \mu > \mu_0$ у нас правостороння критическая область\\
При $H_1: \mu < \mu_0$ у нас левосторонняя критическая область\\
Дальше для этого приводится пример с больницей (нам либо надо просто проверить, что температура не стандартная, либо нам важно знать, что она больше нормы, либо нам важно знать, что она ниже нормы)\\
$P(\sqrt{n}\frac{\overline{X} - \mu_0}{S} \in T_0(\alpha) | \mu \neq \mu_0) = P(\sqrt{n}\frac{\overline{X} - \mu}{S} + \frac{\mu - \mu_0}{S}\sqrt{n} | \mu \neq \mu_0)$\\
Это будет стремиться либо к $\Phi(-\infty)$ либо $1 - \Phi(+\infty)$
\section{Критерии Колмогорова-Смирнова.}
\bf{Критерий Колмогорова}\\
$X_1, \dots, X_n \sim F$\\
$H_0: F = F_0$ ($F_0$ - непр)\\
$H_1: F \neq F_0$\\
Идея основана на теореме Колмогорова (было в начале семестра)\\
$D_n = \sqrt{n}\underset{x \in \mathbb{R}}{sum}|F_n(x) - F_0(x)|$, $F_n$ - эмпирическая функция распределения\\
$\text{if }D_n > q_{1 - \alpha} \text{ then reject } H_0\text{ else accept } H_0$\\
1) $n \geq 20$ работает хорошо, при маленьких n есть спец таблицы\\
2) Так же есть приблеженные формулы для $D_n$\\
3) $H_0: F = F(\theta), H_1: \neg H_0 \Rto D_n = \sqrt{n}\underset{x}{sup}|F_n(x) - F_0(x, \theta)|$\\
$\theta \to \widehat{\theta}$\\
В пределе будет более сложная формула\\
\bf{Критерий Смирнова}\\
$X_1, \dots, X_n$\\
$Y_1, \dots, Y_m$\\
Они независимы\\
$H_0: F_X = F_Y (= F_0)$\\
$H_1: \neq H_0$\\
Тут идея основана на формуле Смирнова\\
$D_{n, m} = \sqrt{\frac{nm}{n + m}}\underset{x}{sup}|F_n(x) - F_m(x)|$\\
$T_1(\alpha) = (q_{1 - \alpha}, +\infty)$
\section{Критерий согласия Пирсона хи-квадрат для простой и сложных гипотез}
$X_1, \dots, X_n \sim F(x)$ - непрерывная\\
Давайте дискритезируем данные\\
$\Delta_1: \nu_1$ - количесво элементов выборки попадающих $\Delta_1$\\
$\dots$\\
$\Delta_N$\\
$p_{\Delta_k} = \displaystyle\int_{\delta_k} p(x) dx$, $p(x) = F'(x)$\\
Рассмотрим $\{1, 2, \dots, N\}$, $p = (p_1, \dots, p_N)$ - настоящий вектор вероятностей\\
$p_0 = (p_{01}, \dots, p_{0N})$ - ожидаемый фиксированный вектор вероятностей\\
$\nu_k$ - количество элементов в выборке типа k\\
$H_0: p = p_0$\\
$H_1: p \neq p_0$\\
$n = \displaystyle\sum_{k = 1}^N \nu_k$\\
$\Xi_N^2 = \displaystyle\sum_{k = 1}^N \frac{(\nu_k - np_{0k})^2}{np_{0k}}$\\
\textbf{Теорема}\\
$\Xi_N^2 \xrto[n \to \infty]{} \Xi^2(N - 1)$ при условии $H_0$\\
\textbf{Доказательство}\\
$N = 2:$\\
$\frac{(\nu_1 - np_{01})^2}{np_{01}} + \frac{(\nu_2 - np_{02})^2}{np_{02}} = \frac{(\nu_1 - np_{01})^2}{np_{01}} + \frac{(n - \nu_1 - n(1 - p_{01}))^2}{n(1 - p_{01})} = \frac{(\nu_1 - np_{01})^2}{n}(\frac{1}{p_{01}} + \frac{1}{1 - p_{01}}) = \frac{(\nu_1 - np_{01})^2}{np_{01}(1 - p_{01})}$\\
Без квадрата по ЦПТ это стремится к N(0, 1), но мы можем навесить непрерывную функцию возведения в квадрат и получим то, что нам нужно\\
Для классического критерий хи-квадрат у нас правосторонняя критическая область (потому что в сумме при $H_0$ будет маленькая разность в квадрате и тд и тп очев крч)\\
Критерий состоятельный (то есть вероятность ошибки второго рода стремистя к единице, если погнать объем выборки на бесконечность)\\
\textbf{Теорема}\\
$p_0(\theta) > 0 \forall \theta$\\
$\frac{\pat p_0}{\pat \theta}, \frac{\pat^2 p_0}{(\pat \theta)^2}$ - непрерывная\\
Тогда $\Xi_N^2 \to X^2(N - 1 - d)$\\
$rk(\frac{\pat p_{0k}}{\pat \theta_j})_{1 \leq k \leq N, 1 \leq j \leq d} = d$\\
\section{Критерий однородности хи-квадрат, в том числе случай 2 $\times$ 2}
Пусть у нас k независимых выборок все они из $\{1, 2, \dots, N\}$\\
Пусть $p^{(j)}$ - истинный вектор вероятностей для соответствующей выборки\\
$H_0: p^{(1)} = \dots = p^{(k)}$\\
$H_1: \neq H_0$\\
$\nu_{ij}$ - количество элементов типа j в i-ой выборке\\
$n_i = \displaystyle\sum_j \nu_{ij} = \nu_{i*}$ - объем i-ой выборки\\
$n = n_1 + \dots + n_k$\\
Пусть $p^{(1)}, \dots, p^{(k)}$ - известны\\
$\Xi_{n_1}^2 = \displaystyle_{j = 1}^N \frac{(\nu_{ij} - n_i p_j^{(i)})^2}{n_i p_j^(i)}, df = N - 1$\\
$\Xi^2_{n_1, n_2, \dots, n_k} = \displaystyle\sum_{i = 1}^K \Xi_{n_i}^2, df = k(N - 1)$\\
Рассмотрим $p^{(1)}, \dots, p^{(k)}$ - не известны\\
$df = k(N - 1) - (N - 1) = (N - 1)(k - 1)$\\
$\widehat{p_{j}} = \frac{\nu_{1j} + \dots + \nu_{kj}}{n} = \frac{\nu_{*j}}{n}$\\
$L(\dots) = p_1^{(\nu_{*1})} \dots p_k^{(\nu_{*k})}, n = \displaystyle\sum_j \nu_{*j}$\\
$Z_{n_1n_2} = (\frac{\nu_{11}}{n_1} - \frac{\nu_{21}}{n_2})\sqrt{\frac{nn_1n_2}{\nu_{*1}\nu_{*2}}}$\\
Утверждается, что $Z_{n_1n_2}^2 = \Xi_{n_1n_2}^2 \to N(0, 1)$\\
$H_0: p_1 = p_2, p_j = P(\text{0 в j-ой выборке})$\\
$H_1$ - настраивается
\section{Критерий независимости хи-квадрат, в том числе случай 2 $\times$ 2}
$X_1, \dots, X_n: \{1, 2, \dots, N\}$\\
$Y_1, \dots, Y_n: \{1, 2, \dots, M\}$\\
$\nu_{ij}$ - количество пар, в которых первая компонента равна i, а вторая j\\
Это можно представить в виде таблицы  сопряженности\\
Проссумируем по каждому столбцу и по каждой строчке\\
Пусть $p_{ij} = P(X = i, Y = j)$\\
$p_{xi} = P(X = i)$\\
$p_{yj} = P(Y = j)$\\
$H_0: p_{ij} = p_{xi} p_{yj} \forall i, j$\\
$H_1: \neg H_0$\\
$\Xi^2 = \displaystyle\sum_{1 \leq i \leq N, 1 \leq j \leq M} \frac{(\nu_{ij} - p_{ij}n)^2}{np_{ij}}, df = MN - 1$\\
$df = MN - 1 - (N - 1) - (M - 1) = MN - N - M + 1 = N(M - 1) - (M - 1) = (M - 1)(N - 1)$\\
$\widehat{p_{Xi}} = \frac{\nu_{i*}}{n}$\\
$\widehat{p_{Yj}} = \frac{\nu_{j*}}{n}$\\
$Z_n = (\frac{\nu_{11}}{\nu_{1*}} - \frac{\nu_{21}}{\nu_{2*}})\sqrt{\frac{n\nu_{1*}\nu_{2*}}{\nu_{*1}\nu_{*2}}}$\\
Утверждается, что $Z_n^2 = \Xi^2$\\
$Z_n = \sqrt{n}\rho_n,$ $\rho_n$ - выборочный коэффициент корреляции Пирсона (ковариация делить на корень из произведения дисперсий ыыыыыыыы)\\
$\rho = \frac{\Cov(X, Y)}{\sigma_X \sigma_Y} = \frac{E(XY) - EXEY}{\sigma_X \sigma_Y} = \frac{P(X = 1, Y = 1) - P(X = 1)P(Y = 1)}{\sqrt{P(X = 1)P(X = 0)P(Y = 1)P(Y = 0)}} = \dots \text{Тренер ну тут очев лол кек кд чд бро} = P(Y = 1 | X = 1) - P(Y = 1 | X = 0)$
\section{Критерий квантилей и знаков}
\subsection{Критерий квантилей}
$H_0:$\\
$F(q_1) = \alpha_1\\
F(q_2) = \alpha_2\\
\dots \\
F(q_N) = \alpha_N$\\
где $\alpha_0 = 0 < \alpha_1 < \dots < \alpha_N < 1 = \alpha_{N + 1}$\\
$H_1: \neg H_0$\\
$q_0 = \inf \supp P < q_1 < \dots < q_N < \sup \supp P = q_{N + 1}$\\
$\Delta_1 = [q_0, q_1)\\
\dots\\
\Delta_{N + 1} = [q_N, q_{N + 1})$\\
$P(\Delta_1) = \alpha_1 - \alpha_0\\
\dots\\
P(\Delta_N) = \alpha_{N + 1} - \alpha_N$\\
\subsection{Критерий знаков}
Это особый случай $H_0: F(q) = \frac{1}{2}$
$(X_1, Y_1)^T, \dots, (X_n, , Y_n)^T$\\
Мы хотим проверить что:\\
a) выборки независимы\\
б) распределения одинаковы\\
$F(x, y) = F_1(x) \cdot F_1(y)$\\
$U = X - Y \Rto \med U = 0$\\
$\nu_1$ - количество элементов новой выборки > med\\
$Z_n = \frac{2}{\sqrt{n}}(\nu_1 - \frac{n}{2}) \to N(0, 1)$\\
$Z_n = \sqrt{n} \rho_n, \rho_n$ - коэффициент корреляции Пирсона\\
\textbf{Теорема}\\
$(X_1, Y_1)^T, \dots, (X_n, Y_n)^T \sim N(\dots, \dots) \Rto \frac{\sqrt{n -2} \rho_n}{\sqrt{1 - \rho_n^2}} \sim T(n - 2)$\\
$H_0: \rho = 0\\
H_1: (\rho \neq 0, \rho > 0, \rho < 0)$
\section{Ранговые критерии. Критерии Манна-Уинтни-Уилкоксона}
\textbf{Определение. Ранг}\\
$X_1, \dots, X_n$ - выборка\\
$r(X_k)$ - номер $X_k$ в вариационном ряде\\
$X_1, \dots, X_n$\\
$Y_1, \dots, Y_m$\\
Это две независиммых выборки, давайте объединим их в одну\\
Рассмотрим $(X_1, \dots, X_n, Y_1, \dots, Y_m)$\\
$R_i$ - ранг $X_i$, в объединенной выборке\\
$T = R_1 + \dots, R_n$ - Статистика Вилкоксона\\
$Z_{rs} = \mathbb{1}(X_r < Y_s)$\\
$U = \displaystyle\sum_{r = 1}^n \displaystyle\sum_{s = 1}^m Z_{rs}$ - Мант-Уитни\\
$T + U = mn + \frac{n(n + 1)}{2}$\\
Хотим проверить, что распределение X совпадает с Y\\
$EU = mn E \mathbb{1}(X < Y) = mnP(X < Y) = \frac{1}{2}$ - при условии $H_0$\\
$H_0: P(X < Y) = \frac{1}{2}$\\
$U \sim N(\frac{mn}{2}, \frac{nm(m + n + 1)}{12})$\\
\textbf{Теорема}\\
$(X_1, Y_1)^T, \dots, (X_n, , Y_n)^T \sim N(\dots, \dots) \Rto \frac{\sqrt{n -2} \rho_n}{\sqrt{1 - \rho_n^2}} \sim T(n - 2)$\\
\section{Коэффициенты корреляции Пирсона, Спирмена и Кендала. Статистические тесты, основанные на них.}
Хотим проверить независимость\\
$R_i$ - ранг $X_i$ (в своей выборке)\\
$S_i$ - ранг $Y_i$ (в своей выборке)\\
$\rho$ - выборочный коэффициент корреляции между $R_i$ и $S_i$ - коэффициент корреляции Спирмена\\
$\rho = \frac{12}{n(n^2 - 1)} \displaystyle\sum (R_i - \frac{n + 1}{2})(S_i - \frac{n + 1}{2})$\\
$H_0: \rho = 0, \sqrt{n}\rho \to N(0, 1)$\\
$H_1: \rho \neq 0, \rho > 0, \rho < 0$\\
\bf{Коэффициент корреляции Кендала}\\
$\tau = \frac{2}{n(n - 1)} \displaystyle\sum_{i = 1}^{n - 1}\displaystyle\sum_{j = i + 1}^{n}\sign(T_j - T_i)$\\
$H_0$ верно $\Rto E\tau = 0, \Var \tau = \frac{2(2n + 5)}{9n(n - 1)}$\\
$\tau \approx N(0, \frac{4}{9n})$\\
$H_0: \tau = 0$\\
$H_1: > 0, < 0, \neq 0$\\
$\rho = \frac{\Cov(X, Y)}{\sigma_X\sigma_Y} = \frac{E((X - \mu_X)(Y - \mu_Y))}{\sigma_X\sigma_Y}$\\
Пирсон проверяет линейную зависимость между двумя случайными величинами\\
Спирмен проверяет монотонную зависимость между двумя случайными величинами\\
Кендал делает тоже самое что и Спирмен\\
\section{Критерий инверсий}
$X_{(1)} \leq \dots \leq X_{(n)}$
Крайние ситуации: выборка отсортированна, то есть трудно поверить, что у нас все случайно\\
$\nu_i$ - количество инверсий для элемента $X_i$\\
$T = \nu_1 + \dots + \nu_{n}$\\
$ET = \frac{n(n - 1)}{4}$\\
$\Var T = \frac{n(n - 1)(2n + 5)}{72}$\\
Статистика $T$ - ассимптотически нормальная\\
\section{Модель линейной регрессии. Минимальные и обычные предположения. Оценка наименьших квадратов.}
$Y = Xb + \varepsilon$\\
$X \in M_{n \times m}(\mathbb{R})$ - матрица переменных\\
$x_{ij}$ - количественная переменная\\
$Y \in \mathbb{R}^n$ - наблюдения зависимой переменной\\
$b \in \mathbb{R}^m$ - неизвестный вектор коэффициентов\\
$\varepsilon \in \mathbb{R}^n$ - случайная ошибка\\
1) $E\varepsilon = 0$\\
2) $\Var \varepsilon_i = \sigma^2$ - гомоскедотичность\\
3) $\Cov(\varepsilon_i, \varepsilon_j) = 0$\\
Наша цель оценить $b$ и $\sigma^2$\\
\bf{Определение. Ошибка наименьших квадратов}\\
$\widehat{b} = \argmin S^2(b)$\\
$S^2(b) = (Xb - Y)^T(Xb - Y)$\\
$A = X^T X \in M(m \times n)$\\
$\frac{1}{n}X^T X$\\
$rank(A) = m$\\
\bf{Теорема}\\
$\widehat{b} = A^{-1}X^T Y$\\
\bf{Доказательство}\\
$S^2(\widehat{b} + \delta) = S^2(\widehat{b}) + \delta^T A \delta$\\ %TODO
$t = T b, T \in M_{k \times m}, k \leq m, rank T = k$\\
$\widehat{t} = T \widehat{b}$\\
\section{Теорема Гаусса-Маркова}
1) $\widehat{t}$ - несмещенная оценка $t$\\
2) $\widehat{t}$ - оптимальная оценка в классе линейных несмещенных оценок\\
\bf{Доказательство}\\
$E\widehat{t} = ET\widehat{b} = ETA^{-1}X^TY = TA^{-1}X^TEY = TA^{-1}X^TXb = Tb = t$\\
$\Var(\widehat{t}) = \Var TA^{-1}X^TY = TA^{-1}X^T \Var Y X A^{-1}T^T = \sigma^2 TA^{-1}T^T$\\
$\Var \widehat{t} = \sigma^2 TA^{-1}T^T$\\
$\Var \widehat{b} = \sigma^2 A^{-1}$\\
Пусть $LY$ - несмещенная оценка для $t$, т.е.\\
$ELY = t = Tb \Rto \Var LY = \sigma^2 L L^T$\\
Заметим, что $LL^T = (TA^{-1}X^T)(TA^{-1}X^T)^T + (L - TA^{-1}X^T)(L - TA^{-1}X^T)^T$\\
$MSE \widehat{t} = tr \Var \widehat{t}$\\
Тогда $L = TA^{-1}X^T$\\
\\
\section{Оценка остаточной дисперсии}
$ES^2(b) = E(Xb - Y)^T(Xb - Y) = E\varepsilon^T\varepsilon = n\sigma^2$\\
$E(\widehat{b} - b)A(\widehat{b} - b) = \displaystyle\sum_{i, j}a_{ij}E(\widehat{b_i} - b_i)(\widehat{b_j} - b_j) = \displaystyle\sum_{i, j}a_{ij}\Cov(\widehat{b_i}, \widehat{b_j}) = \sigma^2 \displaystyle\sum_{i, j}a_{ij}a_{ij}^{-1} = \sigma^2 \displaystyle\sum_{i} \displaystyle\sum_{j}a_{ij}a_{ji}^{-1} = \sigma^2 \displaystyle\sum_{i = 1}^m 1 = \sigma^2 m$\\
$S^2(b) = S^2(\widehat{b}) + (\widehat{b} - b)^TA(\widehat{b} - b) \Rto n\sigma^2 = ES^2(\widehat{b}) + m\sigma^2 \Rto \widehat{\sigma}^2 = \frac{S^2(\widehat{b})}{n - m}$ - несмещенная оценка для $\sigma^2$\\
\section{Условная оценка наименьших квадратов}
$Tb = t_0, T \in M_{k \times m}, rank T = k$\\
$\widehat{b}_{T, t_0} = \underset{Tb = t_0}{\argmin} S^2(b)$ - условная оценка наименьших квадратов\\
\bf{Теорема}\\
$\widehat{b}_{T, t_0} = \widehat{b} - A^{-1}T^TD^{-1}(T\widehat{b} - t_0), D = TA^{-1}T^T$\\
Можно показать: $S^2(b) = S^2(\widehat{b}_{T, t_0}) + (\widehat{b}_{T, t_0} - b)^TA(\widehat{b}_{T, t_0} - b)$\\
\section{Основная теорема о линейной регрессии. Следствия. t-тест.}
1) $S^2(\widehat{b}), \widehat{b}$ - незав.\\
$S^2(b) - S^2(\widehat{b}), S^2(\widehat{b})$ - незав.\\
2) $\widehat{b} \sim N(b, \sigma^2A^{-1})$\\
3) $\frac{S^2(\widehat{b})}{\sigma^2} \sim \Xi(n - m)$\\
$\frac{S^2(b) - S^2(\widehat{b})}{\sigma^2} \sim \Xi^2(m)$\\
Без доказательства\\
T-test:\\
$\frac{\frac{\widehat{b_j} - b_j}{\sigma\sqrt{A^{-1}_{ij}}}}{\sqrt{\frac{1}{n - m} \frac{S^2(\widehat{b})}{\sigma^2}}} \sim T(n - m)$\\
$\sqrt{\frac{1}{n - m} \frac{S^2(\widehat{b})}{\sigma^2}}$ = $\frac{\widehat{b_j} - b_j}{S(\widehat{b}\sqrt{A^{-1}_{ij}})}\sqrt{n - m}$\\
$H_0: b_j = b_{0j}$\\
$H_1: <, \neq, >$\\
\section{F-тест. Коэффициент детерминации.}
$T \in M_{k \times m}$\\
$H_0: Tb = t_0$ (default: T = $E_m$, $t_0$ = 0, то есть все $b_i = 0$ одновременно)\\
$H_1: Tb \neq t_0$\\
$F = \frac{n - m}{k} \cdot \frac{S^2(\widehat{b}_{T, t_0}) - S^2(\widehat{b})}{S^2(\widehat{b})} = \frac{n - m}{k} \cdot \frac{(T\widehat{b} - t_0)^TD^{-1}(T\widehat{b} - t_0)}{S^2(\widehat{b})} \sim F(k, n - m)$\\
$R = \frac{\displaystyle\sum_i (Y_i - \overline{Y})(\widehat{Y_i} - \overline{\widehat{Y}})}{\sqrt{\displaystyle\sum_i (Y_i - \overline{Y})^2\displaystyle\sum_i(\widehat{Y_i} - \overline{\widehat{Y}})^2}}$, $\widehat{Y} = X\widehat{b}$\\
$R$ - многомерный коэффициент корреляции\\
$R^2$ - коэффициент детерминации
\section{Однофакторный дисперсионный анализ}
$Y_{ij} = \mu_j + \varepsilon_{ij}$, $\mu_j$ - среднее влияние на $j-$ом факторе\\
$1 \leq j \leq \nu$ - уровень фактора\\
$1 \leq i \leq I_j$ - i-ое наблюдение для фатора с уровнем j\\
$I = \displaystyle\sum_{j = 1}^\nu I_j$\\
$\varepsilon_{ij} \sim N(0, \sigma^2E)$\\
$H_0: \mu_1 = \mu_2 = \dots = \mu_J$\\
$H_1: \neg H_0$\\
$F = \frac{\frac{S_B^2}{df_B}}{\frac{S_W^2}{df_W}} \sim F(J - 1, I - J)$\\
$S_B^2 = \displaystyle\sum_{j = 1}^\nu I_j (\overline{Y_{*j}} - \overline{Y})^2$ - межгруповая дисперсия\\
$df_B = J - 1$\\
$S_W^2 = \displaystyle\sum_i \displaystyle\sum_j (Y_{ij} - \overline{Y_{*j}})^2$ - внутригруповая дисперсия\\
$df_W = I - J$\\
$S^2 = S_W^2 + S_B^2$\\
\section{Двухфакторный дисперсионный анализ.}
$Y_{ij} = \mu + a_i + b_j + \varepsilon_{ij}$\\
$1 \leq i \leq I$\\
$1 \leq j \leq J$\\
Для каждой пары (i, j) ровно одно наблюдение\\
M - общее среднее\\
$a_i$ - среднеее влияние на i-ом уровне фактора 1\\
$b_j$ - среднеее влияние на j-ом уровне фактора 2\\
Факторы независимы\\
$\displaystyle\sum a_i = \displaystyle\sum b_j = 0$\\
$\varepsilon_{ij}$ - незав.\\
$\varepsilon_{ij} \sim N(0, \sigma^2)$\\
$H_A: \forall a_i = 0 \to F_A = \frac{(I - 1)(J - 1)}{I - 1} \frac{S^2_A}{S^2_{ALL}}$\\
$H_B: \forall b_i = 0 \to F_B = \frac{(I - 1)(J - 1)}{J - 1} \frac{S^2_B}{S^2_{ALL}}$\\
$H: \forall a_i = b_j = 0 \to F = \frac{(I - 1)(J - 1)}{I + J - 2}\frac{S^2_A + S^2_B}{S^2_{ALL}}$\\
$S^2_A = J \displaystyle\sum_i (\overline{Y_{i*}} - \overline{Y})^2$\\
$S^2_B = I \displaystyle\sum_j (\overline{Y_{*j}} - \overline{Y})^2$\\
$S^2_{ALL} = \displaystyle\sum_{i,j}(Y_{ij} - \overline{Y_{i*}} - \overline{Y_{*j}} + \overline{Y})^2$
\section{Ковариационный анализ.}
1 фактор, 1 количественная переменная\\
$y_{ij} = \beta_i + \gamma z_{ij} + \varepsilon_{ij}$\\
$1 \leq i \leq I$\\
$1 \leq j \leq J$\\
$n = I \cdot J$\\
На каждой паре (i, j) одно наблюдение\\
$\beta_i$ - среднее влияние на уровне i\\
$z_{ij}$ - количественная переменная\\
$\varepsilon_{ij}$ - нез.\\
$\varepsilon_{ij} \sim N(0, \sigma^2)$\\
$H_\gamma: \gamma = 0$\\
$\widehat{\beta}, \widehat{\gamma}$ - ОНК $\Rto \sqrt{n - I - 1}\frac{\widehat{\gamma}}{\sqrt{A^{-1}_{\gamma\gamma}}S^2(\widehat{\beta}, \widehat{\gamma})} \sim T(n - I - 1)$\\
$A_{\gamma\gamma} = A_{(I + 1)(I + 1)}$\\
$S^2(\widehat{\beta}, \widehat{\gamma}) = \displaystyle\sum_{ij}(y_{ij} - \widehat{\beta_i} - \widehat{\gamma}z_{ij})^2$ - суммарная квадратичная ошибка\\
$H_B: \beta_1 = \dots = \beta_I$\\
$F = \frac{n - I - 1}{I - 1} \cdot \frac{\displaystyle\sum_{i,j} y^2_{ij} - n\overline{y}^2 - p}{p}$\\
$p = \displaystyle\sum_{i,j}y^2_{ij} - n\overline{y} - \frac{(\displaystyle\sum_{i,j}z_{ij}y_{ij} - n\overline{z})^2}{\displaystyle\sum_{i,j}z^2_{ij} - n\overline{z}}$
\section{Логистическая регрессия}
$Y_i \approx \frac{1}{1 + e^{-x_ic}}$, $X = (X_1 \dots X_n)^T$ $c = (c_1 \dots c_m)^T$\\
$Y_i \sim Bern(\frac{1}{1 + e^{-x_ic}})$\\
$L(X, c) = \displaystyle\prod_{i = 1}^n(\frac{1}{1 + e^{-x_ic}})^{Y_i}(1 - \frac{1}{1 + e^{-x_ic}})^{1 - Y_i}$\\
$-\ln{L(X, c)} = \dots$\\
\section{Последовательный анализ Вальда. Описание алгоритма. Теорема о конечном числе шагов
для простых гипотез.}
$H_0: F = F_0$\\
$H_1: F = F_1$
$f_0, f_1$ - соответствующие плотности
$A_0 < 1 < A_1\\
n = 1$\\
$\text{while true }\{$\\
$\text{if }\frac{L_{0n}}{L_{1n}} \leq A_0 \text{ then return reject }H_0$\\
$\text{if }\frac{L_{0n}}{L_{1n}} \geq A_1 \text{ then return accept }H_0$\\
$++n$
$\}$

$\alpha, \beta, a_0 = \ln{A_0}, a_1 = \ln{A_1}$\\
$Z_i = \ln{\frac{f_1(X_i)}{f_0(X_i)}}$\\
$\displaystyle\sum_{i = 1}^{n} Z_i = \ln{\frac{L_{1n}}{L_{0n}}}$\\
\\
\bf{Теорема}\\
Пусть $\nu$ - количество итераций\\
$\Rto P(\nu \geq n | H_0) \to 0, P(\nu \geq n | H_1 \to 0), n \to \infty$\\
\bf{Доказательство}\\
$r - fixed$\\
$\eta_1 = Z_1 + \dots + Z_r$\\
$\eta_1 = Z_{r + 1} + \dots + Z_{2r}$\\
$\dots$
$\{\nu \geq rk\} \LRto \{a_0 < \eta_1 + \dots + \eta_j < a_1\}_{i \leq j \leq k} = \{a_0 < \eta_1 < a_1, a_0 < \eta_1 + \eta_2 < a_1, \dots, a_0 < \eta_1 + \dots + \eta_k < a_1\} \subset \{|\eta_j| < b = a_i - a_0\}_{1 \leq j \leq k} \Rto P(\nu \geq rk | H_s) \leq P_{1 \leq j \leq k}(|\eta_j| < b | H_s) = P_{1 \leq j \leq k}(\nu_j^2 < b^2 | H_s) = P^k(\eta_1^2 < b^2 | H_s) = p_s^k$\\
$E_s\eta_1^2 \geq \Var_s\eta^1 = r\Var_s Z_1 > b^2, r > \max{(\frac{b^2}{\Var(Z_1 | H_0)}, \frac{b^2}{\Var{Z_1 | H_1}})_{\Var_s(\dots) = \Var(\dots | H_s)}^{E_s(\dots) = E(\dots | H_s)}} \Rto p+s < 1$\\
$P(\nu > n | H_s) \leq P(\nu \geq rk | H_s) \leq p_s^k \to 0, k = k(n), n \to \infty$\\
\section{Последовательный анализ Вальда. Теорема об оценке граничных констант и вероятностей
ошибок первого и второго рода. Оценка среднего числа итераций.}
\bf{Теорема}\\
Пусть $(\alpha, \beta)$ - вероятности $\Rto A_0 \geq A_0' = \frac{\beta}{1 - \alpha}$ и $A_1 \leq A_1' = \frac{1 - \beta}{\alpha}$\\
$\text{if } A_0'$ и $A_1 = A_1' \text{ then } \alpha', \beta'$ - вероятности ошибок\\
$\alpha' \leq \frac{\alpha}{1 - \beta}, \beta' \leq \frac{\beta}{1 - \alpha}$\\
$\alpha' + \beta' \leq \alpha + \beta$\\
\bf{Доказательство}\\
$1 = \displaystyle\sum_{n = 1}^{\infty}P(\nu = n | H_0) = \displaystyle\sum_{n = 1}^{\infty}P(\text{accept at the step n | }H_0) + \displaystyle\sum_{n = 1}^{\infty}P(\text{reject at the step n | }H_0) = \alpha = \displaystyle\sum_{n = 1}^{\infty}P(\text{reject at the step n |}H_0) \leq \frac{1}{A_1}\displaystyle\sum_{n = 1}^{\infty}P(\text{reject at the step n | }H_1)$\\
$L_{n1} \geq L_{n0} \cdot A_1$\\
$L_{n1 \geq L_{n0} \cdot A_1}$\\
Аналогичным образом мы можем получить неравенство:\\
$\beta \leq A_0(1 - \alpha)$\\
$\alpha \leq \frac{1}{A_1}(1 - \beta)$\\
Пусть $A_0' = \frac{\beta}{1 - \alpha}$ и $A_1' = \frac{1 - \beta}{\alpha}$ и ошибки $(\alpha', \beta') \Rto \frac{\beta}{1 - \alpha} \geq \frac{\beta'}{1 - \alpha'}$\\
$\frac{1 - \beta}{\alpha} \leq \frac{1 - \beta'}{\alpha'} \Rto \alpha' \leq \frac{\alpha(1 - \beta')}{1 - \beta} \leq \frac{\alpha}{1 - \beta}$\\
$\beta(1 - \alpha) \leq \beta(1 - \alpha')$\\
$(1 - \beta)\alpha' \leq (1 - \beta')\alpha$\\
Сложим последние два неравества и получим требуемое\\
Давайте вспомним тождество Вальда:\\
$(X_i)$ - независимые случайные величины с математическим ожиданием a\\
$\nu$ - целочисленная случайная величина не зависящая от $X_i$\\
$E\nu = b$\\
$S_\nu = X_1 + \dots + X_\nu$\\
$\Rto ES_\nu = ab = E(ES_\nu | \nu) = E\nu a = aE\nu = ab$\\
$a_0 = \ln{\frac{\beta}{1 - \alpha}}, a_1 = \ln{\frac{1 - \beta}{\alpha}}$\\
$E_s\nu \cdot E_sZ_i = E_sS_\nu \approx a_0(1 - \alpha) + a_1(1 - \beta)$\\
$E_1\nu \cdot E_1Z_i \approx \beta a_0 + a_1(1 - \alpha)$
\section{Бутстреп. Оценка дисперсии, построение д.и., permutation test.}
\begin{enumerate}
    \item $X_1, \dots, X_n \sim F_{\theta}; \widehat{\theta}; \Var \widehat{\theta} - ?$ Можно ли ее оценить численно?
    \item $X_1, \dots, X_n; \overline{X}; \Var \overline{X} = \frac{\sigma}{\eta}$ можно ли оценить численно?
    \item $X_1, \dots, X_n; \med X; \Var \med(X) - ?$\\
\end{enumerate}
$X-1, \dots, X_n; \widehat{\theta}$ - оценка чего-то; $F_n$ - Э.Ф.П\\
$\text{for (int i = 0; i <= B - 1; ++i)}$ \{\\
\text{  }$X_1^*, \dots, X_n^* \sim F_n$\\
\text{  }$\theta_i^* = g(X_1^*, \dots, X_n^*)$\\
\text{  }$\theta^*.append(\theta^*)$\\
\}\\
$\Var^* = \frac{1}{B} \displaystyle\sum_{i = 0}^{B - 1}(\theta_I^* - \overline{\theta_i^*})^2$\\
$\frac{\Var^*}{\Var \widehat{\theta}} \xrto[n \to \infty]{P} 1$\\
\subsection{CI}
$\widehat{F(t)} = \frac{1}{B}\displaystyle\sum_{j = 1}^{B} \mathbb{1}(\sqrt{n}(\overline{\theta_i^*} - \widehat{\theta}) \leq t)$\\
$\widehat{\theta} \pm \frac{q_1 - \frac{\alpha}{2}}{\sqrt{n}}; q_{1 - \frac{\alpha}{2}}$ - квантиль $\widehat{F}$\\
$\widehat{\theta} - \frac{q_{\frac{\theta}{2}}}{\sqrt{n}}; \widehat{\theta} + \frac{q_{1 - \frac{\alpha}{2}}}{\sqrt{n}}$
\subsection{The permutation test}
$X_1, \dots, X_n \sim F$\\
$Y_1, \dots, Y_m \sim G$\\
Both in dependent\\
$H_0: F = G$\\
$H_1: F \neq G$\\
$W = (X_1, \dots, X_n, Y_1, \dots, Y_m)$\\
$X_1 \dots X_n Y_1 \dots, Y_m$\\
$W_1\dots W_{n + m}$\\
Пусть $T = T(X, Y)$ - некая статистика\\
$T = |\overline{X} - \overline{Y}|$\\
$T(W, Z) = |\overline{(Z, W)} - \overline{Y(Z, W)}|$\\
$X(Z, W) = \{W_i : Z_i = 1\}$\\
$Y(Z, W) = \{W_i : Z_i = 2\}$\\
Пусть $Z^*$ - перестановка Z\\
$\text{for i in range(B):}$\\
$\text{    }Z^*$ - перестановка\\
$\text{    }T^* = T(Z^*, W)$\\
$\text{    }T_{all}.append(T^*)$\\
$p = \frac{1}{B}\displaystyle\sum_{j = 1}^B \mathbb{1}(T_{all}[j] > t), t = T(W, Z)$\\
$p = p_{value}$\\
\section{Введение в байесовскую статистику. Credible intervals. Проверка гипотез.}
\subsection{Credible interval}
$P(a_l < \theta < a_r | X) = 1 - \alpha$\\
$\theta \sim \Gamma(\theta)$\\
$p(\theta | X) = \frac{p(X | \theta) \cdot \pi(\theta)}{\displaystyle\int p(X | \theta) \pi(\theta) d\theta}$\\
\subsection{Hypothesis. Bayesian}
$H_0: \theta = \theta_0 - p_0 - prior$\\
$H_1: \theta = \theta_1 - p_1 - prior$\\
$L(\theta_0 | X) = \frac{L(X | \theta_0) \cdot p_0}{const}$\\
$const = P(X) = L(X | \theta_0) \cdot p_0 + L(X | \theta_1) \cdot p_1$\\
$L(\theta_1 | X) \approx L(X | \theta_1) \cdot p_1$\\
$Y = Xc + \varepsilon, C \sim N(.,.)$\\
$\varepsilon \sim N(0, \sigma^2I)$\\
$(c, \varepsilon) \sim (.,.)$\\
$(Y | C) \sim N(Xc, \sigma^2I) \Rto L(c | Y) \sim N(.,.)$\\
$\widehat{c} = E(c | Y)$
\end{document}